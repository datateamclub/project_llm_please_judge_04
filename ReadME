# Project 2: Contradictory, My Dear Watson

For the second UM Data Team Club project we will be diving into Natural Language Inference (NLI). This time, our goal is to explore and analyze the relationships between different pairs of text to understand if one sentence logically follows from another. To do so, we will be attempting to solve the Contradictory, My Dear Watson challenge from Kaggle. The goal of this project is to perform two NLI models. The first one using the **BERT** (Bidirectional Encoder Representations from Transformers) model from preset, and the second one using the same model but doing **Transfer Learning**.

# Project 5:  LLMs - You Can't Please Them All KAGGLE COMPETITION

In this project, we explore the robustness of Large Language Models (LLMs) when used as evaluators of essay quality. Specifically, we participate in the Kaggle competition "[LLMs - You Can't Please Them All](https://www.kaggle.com/competitions/llms-you-cant-please-them-all)", which challenges participants to identify exploits in an LLM-as-a-judge system designed to evaluate essays.

## Challenges

Throughout this project, we encountered several challenges:

- **Understanding LLM Evaluation Biases**: Recognizing potential biases in LLMs when they function as judges, such as favoring certain positions or verbosity.

- **Designing Effective Exploits**: Crafting essays that could exploit weaknesses in the LLM-as-a-judge system required creativity and a deep understanding of LLM behavior.

- **Ensuring Generalizability**: Developing strategies that not only exploit specific weaknesses but also generalize across different prompts and LLM configurations.

## The Approach

Our approach focused on identifying and exploiting potential biases and weaknesses in the LLM-as-a-judge system:

1. **Identifying Biases**: We analyzed the LLM's evaluation patterns to detect biases, such as favoring longer responses or specific argument structures.

2. **Crafting Targeted Essays**: Based on identified biases, we crafted essays designed to exploit these tendencies, aiming to receive higher evaluations from the LLM judge.

3. **Iterative Testing**: We employed an iterative process, continuously refining our essays based on feedback from the LLM evaluations to maximize the exploitation of identified weaknesses.

## Competition Controversy: Accidental Code Leak and Its Impact

During the "LLMs - You Can't Please Them All" competition, an incident occurred where a top-ranking team's code was accidentally leaked. This unintended disclosure led to widespread replication of their strategy by other participants, significantly altering the competition's dynamics.

The accidental leak sparked controversy within the Kaggle community. While some viewed the dissemination of the code as an opportunity to level the playing field, others expressed concerns about fairness and the integrity of the competition. Discussions highlighted that teams investing substantial time and effort into original research felt compelled to adopt the leaked strategies to remain competitive, which they perceived as inequitable.

This event underscored the challenges in maintaining a fair competitive environment in data science contests, especially when unintentional sharing of effective methodologies can lead to rapid homogenization of approaches among participants.

## Key Lessons

Participating in this competition provided valuable insights:

- **LLM Vulnerabilities**: Even advanced LLMs can exhibit biases and vulnerabilities when used as evaluators, highlighting the need for robust evaluation mechanisms.

- **Adversarial Thinking**: Approaching the problem from an adversarial perspective allowed us to uncover weaknesses that might not be evident through standard testing.

- **Ethical Considerations**: Exploiting LLM weaknesses raises ethical questions about the deployment of AI systems in evaluative roles, emphasizing the importance of developing fair and unbiased AI.


## References

- [LLMs - You Can't Please Them All | Kaggle](https://www.kaggle.com/competitions/llms-you-cant-please-them-all)
- [Kaggle Competition Repository](https://github.com/zixi-liu/LLMs-You-Cant-Please-Them-All)



This project highlights the complexities and challenges of using LLMs as evaluators and emphasizes the need for continuous research to develop more robust and fair AI systems.
